# Visually-Grounded-Library-of-Behaviors
## Codes coming soon!
## Abstract
  We propose a visually-grounded library of behav-iors  approach  for  manipulating  diverse  objects  across  varyinginitial  and  goal  configurations  and  camera  placements.  Ourkey  innovation  is  to  disentangle  the  standard  image-to-actionmapping  into  two  separate  modules  that  use  different  typesof  perceptual  input:  (1)  a  behavior  selector  which  conditionson  thestaticobject  properties  to  select  the  behaviors  that  cansuccessfully  perform  the  desired  tasks  on  the  object  in  hand,and (2) a library of behaviors each of which conditions on thedynamicobject  properties  to  predict  actions  to  execute  overtime.  The  selector  uses  a  semantically-rich  3D  object  featurerepresentation  extracted  from  images  using  geometry-aware2D-to-3D  neural  networks. 

-------

![Overview](https://github.com/YunchuZhang/Visually-Grounded-Library-of-Behaviors/blob/main/image/overview.png)
